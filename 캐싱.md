# 1. 캐싱(Caching)

캐싱(Caching)은 캐시(Cache)라고 하는 좀 더 빠른 메모리 영역으로 데이터를 가져와서 접근하는 방식을 말한다. 예를 들어 속도가 느린 하드디스크의 데이터를 메모리로 가지고 와서 메모리 상에서 읽기 쓰기를 수행하는 것을 '데이터를 메모리에 캐싱한다'라고 한다. 마찬가지로 메모리 상에 있는 데이터에 연산을 수행하기 위해서 더 빠른 메모리인 CPU 메모리 캐시로 데이터를 가지고 와서 연산을 수행하는 동작도 캐싱을 한다고 표현한다.

캐시는 데이터의 지역성(Locality)이라는 특성을 이용해서 성능 개선을 달성한다. 지역성에는 공간 지역성과 시간 지역성이 존재하는데, 공간 지역성은 한 번 접근한 데이터의 인근에 저장되어 있는 데이터가 다시 접근될 가능성이 높은 특성을 의미한다. 시간 지역성은 한 번 접근된 데이터가 가까운 시간 내에 다시 접근될 가능성이 높다는 의미다.

이러한 데이터 접근의 지역성이라는 특성을 이용해서 자주 접근될 데이터를 더 빠른 속도의 메모리상에 가지고 와서 연산을 수행하여 성능을 높이는 것이 캐싱의 목표다.

## 캐싱은 어떻게 작동?

캐시의 데이터는 일반적으로 RAM(Random Access Memory)과 같이 빠르게 액세스할 수 있는 하드웨어에 저장되며, 소프트웨어 구성 요소와 함께 사용될 수도 있다. 캐시의 주요 목적은 더 느린 기본 스토리지 계층에 액세스해야 하는 필요를 줄임으로써 데이터 검색 성능을 높이는 것이다.

속도를 위해 용량을 절충하는 캐시는 일반적으로 데이터의 하위 집합을 일시적으로 저장한다. 보통 완전하고 영구적인 데이터가 있는 데이터베이스와는 대조적이다.

캐시의 데이터는 일반적으로 RAM(Random Access Memory)과 같이 빠르게 액세스할 수 있는 하드웨어에 저장되며, 소프트웨어 구성 요소와 함께 사용될 수도 있다. 캐시의 주요 목적은 더 느린 기본 스토리지 계층에 액세스해야 하는 필요를 줄임으로써 데이터 검색 성능을 높이는 것이다.

속도를 위해 용량을 절충하는 캐시는 일반적으로 데이터의 하위 집합을 일시적으로 저장한다. 보통 완전하고 영구적인 데이터가 있는 데이터베이스와는 대조적이다.

## 캐싱 개요

### RAM 및 인 메모리 엔진
RAM 및 인 메모리 엔진에서 지원하는 높은 요청 비율 또는 IOPS(초당 입력/출력 작업) 덕분에 캐싱을 사용하면 대규모로 데이터 검색 성능이 향상되고 비용이 절감된다. 기존 데이터베이스 및 디스크 기반 하드웨어로 동일한 규모를 지원하려면 추가 리소스가 필요하게 된다. 이러한 추가 리소스는 비용을 상승시키면서도 여전히 인 메모리 캐시에서 제공하는 짧은 지연 시간 성능은 달성할 수 없다.

### 적용
캐시는 운영 체제, 네트워킹 계층(콘텐츠 전송 네트워크(CDN), DNS 등), 웹 애플리케이션 및 데이터베이스를 비롯한 다양한 기술 계층에 걸쳐 적용되고 활용될 수 있다. 캐싱을 사용하면 Q&A 포털, 게임, 미디어 공유 및 소셜 네트워킹과 같은 읽기 집약적인 여러 애플리케이션 워크로드의 지연 시간을 크게 줄이고 IOPS를 개선할 수 있다. 캐싱되는 정보로는 데이터베이스 쿼리 결과, 컴퓨팅 집약적인 계산, API 요청-응답 및 HTML, JavaScript 및 이미지 파일과 같은 웹 아티팩트가 있다. 또한 추천 엔진과 고성능 컴퓨팅 시뮬레이션과 같이 데이터 세트를 조작하는 컴퓨팅 집약적 워크로드는 캐시 역할을 하는 인 메모리 데이터 계층의 이점을 활용할 수 있다. 이러한 애플리케이션에서는 수백 개의 노드에 걸쳐 있는 시스템 클러스터에서 매우 큰 데이터 세트를 실시간으로 액세스해야 한다. 기본 하드웨어의 속도 때문에 디스크 기반 스토어에서 이 데이터를 조작하는 것은 이러한 애플리케이션에서 심각한 병목 현상을 초래한다.

### 설계 패턴
분산 컴퓨팅 환경에서, 전용 캐싱 계층을 사용하면 시스템과 애플리케이션이 캐시에 영향을 주는 위험 부담 없이 자체 수명 주기를 통해 캐시와는 독립적으로 실행될 수 있다. 캐시는 자체적인 수명 주기 및 아키텍처 토폴로지를 사용하여 서로 다른 시스템에서 액세스할 수 있는 중앙 계층 역할을 한다. 특히 애플리케이션 노드를 동적으로 확장 및 축소할 수 있는 시스템에 적합하다. 캐시를 사용하는 애플리케이션 또는 시스템과 동일한 노드에 캐시가 상주하는 경우, 이 같은 확장과 축소가 캐시의 무결성에 영향을 줄 수 있다. 또한 로컬 캐시를 사용하는 경우 데이터를 소비하는 로컬 애플리케이션에만 도움된다. 분산 캐싱 환경에서는 여러 캐시 서버에 분산된 데이터가 중앙 위치에 저장되므로 해당 데이터의 모든 소비자가 이익을 얻을 수 있다.

### 캐싱 모범 사례
캐시 계층을 구현할 때는 캐싱되는 데이터의 유효성을 이해하는 것이 중요하다. 성공적인 캐시는 높은 적중률로 이어진다. 즉, 가져온 데이터가 캐시에 존재한다. 가져온 데이터가 캐시에 존재하지 않을 때 캐시 비적중이 발생한다. TTL(Time To Live)과 같은 제어 항목을 적용하여 이에 따라 데이터를 만료되도록 할 수 있다. 다른 고려 사항은 캐시 환경이 고가용성이어야 할 필요가 있는지 여부인데, 이는 Redis와 같은 인 메모리 엔진을 사용하여 충족할 수 있다. 기본 위치에서 데이터를 캐싱하는 것과는 달리, 경우에 따라 인 메모리 계층을 독립형 데이터 스토리지 계층으로 사용할 수 있다. 이 시나리오에서는 인 메모리 엔진에 상주하는 데이터에 대해 적절한 RTO(복구 목표 시간 – 가동 중단으로부터 복구하는 데 걸리는 시간)와 RPO(목표 복구 시점 – 복구 시 캡처된 최종 시점 또는 트랜잭션)를 정의하여 이것이 적합한지를 파악하는 것이 중요하다. 다른 인 메모리 엔진의 설계 전략 및 특성들을 적용하면 대부분의 RTO와 RPO 요구 사항을 충족시킬 수 있다.

![image](https://user-images.githubusercontent.com/62053017/225205389-c841f051-7699-48ba-833e-cf189fa8f3ed.png)

| **계층** | **클라이언트 측** | **DNS** | **웹** | **앱** | **데이터베이스** |
| --- | --- | --- | --- | --- | --- |
| **사용사례** | 웹 사이트에서 웹 컨텐츠를 검색하는 속도 가속화(브라우저 또는 디바이스) | 도메인과 IP 간 확인 | 웹 또는 앱 서버에서 웹 컨텐츠를 검색하는 속도를 높인다. 웹 세션 관리(서버 측) | 애플리케이션 성능 및 데이터 액세스 가속화 | 데이터베이스 쿼리 요청과 관련한 지연 시간 단축 |
| **기술** | HTTP 캐시 헤더, 브라우저 | DNS 서버 | HTTP 캐시 헤더, CDN 역방향 프록시, 웹 액셀러레이터, 키-값 스토어 | 키-값 데이터 스토어, 로컬 캐시 | 데이터베이스 버퍼, 키-값 데이터 스토어 |
| **솔루션** | 브라우저별 | Amazon Route 53 | Amazon CloudFront, Redis, Memcached | 애플리케이션 프레임워크, Redis, Memcached | Redis, Memcached |

## 캐싱의 이점

### 애플리케이션 성능 개선
메모리는 디스크(마그네틱 또는 SSD)보다 훨씬 속도가 빠르기 때문에 인 메모리 캐시에서 데이터를 읽는 속도가 매우 빠르다(밀리초 미만). 이렇게 훨씬 더 빠른 데이터 액세스는 애플리케이션의 전반적인 성능을 개선한다.

### 데이터베이스 비용 절감
단일 캐시 인스턴스는 수십만 IOPS(초당 입출력 작업)를 제공할 수 있으며, 따라서 수많은 데이터베이스 인스턴스를 대체할 수 있으므로 총 비용이 절감된다. 이 같은 비용 절감 이점은 기본 데이터베이스에서 처리량당 요금이 부과되는 경우 특히 크다. 이 경우 수십 퍼센트의 비용이 절감될 수 있다.

### 백엔드의 로드 감소
캐싱은 읽기 로드의 상당 부분을 백엔드 데이터베이스에서 인 메모리 계층으로 리디렉션함으로써 데이터베이스의 로드를 줄이고 로드 시에 성능이 저하되거나 작업 급증 시에 작동이 중단되지 않도록 보호할 수 있다.

### 예측 가능한 성능
모던 애플리케이션의 일반적인 과제 중 하나는 애플리케이션 사용량이 급증하는 시기에 대응하는 것이다. 슈퍼볼이나 선거 기간 동안의 소셜 앱, 블랙 프라이데이 기간 동안의 전자 상거래 웹 사이트 등을 예로 들 수 있다. 데이터베이스에 대한 로드가 증가하면 데이터를 가져오는 데 있어 지연 시간이 길어지고 전반적인 애플리케이션 성능이 예측 불가능해진다. 높은 처리량의 인 메모리 캐시를 활용함으로써 이 문제를 완화할 수 있다.

### 데이터베이스 핫스팟 제거
많은 애플리케이션에서 유명인사 프로필이나 인기 있는 제품과 같은 작은 데이터 하위 집합이 나머지 부분에 비해 더 자주 액세스될 것이다. 이로 인해 데이터베이스에 핫스팟이 발생할 수 있으며 가장 자주 사용되는 데이터의 처리량 요구 사항에 맞추어 데이터베이스 리소스를 초과 프로비저닝해야 할 수 있다. 인 메모리 캐시에 공통 키를 저장하면 가장 자주 액세스하는 데이터에 대해 예측 가능한 빠른 성능을 제공하는 동시에 초과 프로비저닝의 필요성을 줄일 수 있다.

### 읽기 처리량(IOPS) 증가
인 메모리 시스템은 지연 시간을 줄일 뿐만 아니라 유사한 디스크 기반 데이터베이스에 비해 훨씬 높은 요청 속도(IOPS)를 제공한다. 분산형 사이드 캐시로 사용되는 단일 인스턴스는 초당 수십만 건의 요청을 처리할 수 있다.

# 사용 사례

## 데이터베이스 캐싱

속도와 처리량 면에서, 데이터베이스가 제공하는 성능은 애플리케이션 전체 성능에 무엇보다 크게 영향을 미칠 수 있다. 또한 오늘날 많은 데이터베이스가 비교적 우수한 성능을 제공하지만, 애플리케이션에 더 높은 성능이 요구되는 사용 사례도 많다. 데이터베이스 캐싱을 사용하면 처리량을 크게 높이고 백엔드 데이터베이스와 관련한 데이터 검색 지연 시간을 줄일 수 있으므로 애플리케이션의 전반적인 성능이 향상된다. 캐시는 성능을 높이기 위해 애플리케이션에 사용할 수 있는 데이터베이스에 인접한 데이터 액세스 계층 역할을 한다. 데이터베이스 캐시 계층은 관계형 데이터베이스와 NoSQL 데이터베이스를 비롯하여 모든 유형의 데이터베이스의 프런트에 적용할 수 있다. 캐시에 데이터를 로드하는 데 사용되는 일반적인 기법으로는 지연 로딩 및 라이트 스루 방식이 있다.

![image](https://user-images.githubusercontent.com/62053017/225205439-d68c2e9b-453b-4ead-9f28-697df61f3f41.png)

## 콘텐츠 전송 네트워크(CDN)

웹 트래픽이 지리적으로 분산되어 있는 경우, 전 세계에 걸쳐 전체 인프라를 복제하는 방식이 현실적으로 불가능할 수도 있고 비용상 효율적이지도 않다. CDN은 글로벌 엣지 로케이션 네트워크를 활용하여 동영상, 웹 페이지, 이미지 등 웹 콘텐츠의 캐싱된 복사본을 클라이언트에게 제공하는 기능을 제공한다. 응답 시간을 줄이기 위해 CDN은 클라이언트와 가장 가까운 엣지 로케이션 또는 원래 요청 위치를 활용함으로써 응답 시간을 단축한다. 웹 자산이 캐시에서 제공되므로 처리량이 대폭 늘어난다. 동적 데이터의 경우 많은 수의 CDN을 구성하여 오리진 서버에서 데이터를 검색할 수 있다.

![image](https://user-images.githubusercontent.com/62053017/225205475-ad733bce-2382-42f0-a825-ce59b30cba5e.png)

## DNS(Domain Name System) 캐싱

인터넷의 모든 도메인 요청에서는 도메인 이름과 연결된 IP 주소를 확인하기 위해 DNS 캐시 서버를 쿼리한다. DNS 캐싱은 OS를 비롯한 여러 수준에서 ISP 및 DNS 서버를 통해 실행될 수 있다.

## 세션 관리

HTTP 세션에는 로그인 정보, 쇼핑 카트 목록, 이전에 본 항목 등, 사이트 사용자와 웹 애플리케이션 간에 교환된 사용자 데이터가 포함된다. 웹 사이트에서 우수한 사용자 환경을 제공하는 데 있어서는 사용자의 기본 설정을 기억하고 풍부한 사용자 컨텍스트를 제공함으로써 HTTP 세션을 효과적으로 관리하는 것이 중요하다. 중앙 집중식 세션 관리 데이터 스토어를 활용하는 것이 모든 웹 서버에서 일관된 사용자 환경을 제공하고, 탄력적인 웹 서버 플릿에서 더 나은 세션 내구성을 제공하며, 세션 데이터가 여러 캐시에 복제되는 경우에 더 높은 가용성을 제공하는 등 여러 가지 이유로 모던 애플리케이션 아키텍처에 이상적인 솔루션이라고 할 수 있다.

![image](https://user-images.githubusercontent.com/62053017/225205528-1ec462b0-fd79-4598-a938-88c457cc2dc3.png)

## 애플리케이션 프로그래밍 인터페이스(API)

오늘날 대부분의 웹 애플리케이션은 API를 기반으로 구축된다. API는 일반적으로 HTTP를 통해 액세스할 수 있으며 사용자가 애플리케이션과 상호 작용할 수 있도록 리소스를 노출하는 RESTful 웹 서비스다. API를 설계할 때 API의 예상 로드, 권한 부여, 버전 변경이 API 소비자에게 미치는 영향, 그리고 무엇보다 API의 사용 편의성 등을 고려하는 것이 중요하다. API가 비즈니스 로직을 인스턴스화하고 모든 요청에서 데이터베이스로 백엔드 요청을 보내야 하는 것은 아니다. 경우에 따라서는 API의 캐싱된 결과를 제공하는 것이 비용 효율적인 최적의 응답 방식이 될 수 있다. 기반 데이터의 변경 속도에 맞춰 API 응답을 캐싱할 수 있는 경우 특히 그렇다. 예를 들어 제품 리스팅 API를 사용자에게 노출했으며 제품 범주는 하루에 한 번만 변경된다고 가정해보겠다. 제품 범주 요청에 대한 응답은 하루 종일 API에 대한 호출이 있을 때마다 동일하므로, 하루 동안 API 응답을 캐싱하는 것으로 충분하다. API 응답을 캐싱함으로써 애플리케이션 서버 및 데이터베이스를 비롯한 인프라에 가해지는 부담을 줄일 수 있다. 또한 응답 시간이 빨라져 더 높은 성능의 API를 제공할 수 있다.

## 하이브리드 환경의 캐싱

하이브리드 클라우드 환경에는 클라우드에 상주하면서 온프레미스 데이터베이스에 자주 액세스해야 하는 애플리케이션이 있을 수 있다. VPN 및 Direct Connect를 비롯하여, 클라우드와 온프레미스 환경 간의 연결을 구현하기 위해 사용할 수 있는 다양한 네트워크 토폴로지가 있다. 또한 VPC에서 온프레미스 데이터 센터까지의 지연 시간이 짧더라도, 전체 데이터 검색 성능을 가속화하기 위해 클라우드 환경에서 온프레미스 데이터를 캐싱하는 것이 가장 바람직할 수도 있다.

## 웹 캐싱

최종 사용자에게 웹 콘텐츠를 제공할 때, 아티팩트를 캐싱하여 디스크 읽기와 서버 로드를 배제하면 이미지, html 문서, 동영상 등의 웹 자산 검색과 관련된 대부분의 지연 시간을 크게 줄일 수 있다. 서버와 클라이언트 양쪽 모두에서 다양한 웹 캐싱 기술을 활용할 수 있다. 서버 측 웹 캐싱은 일반적으로 프런트에 위치한 웹 서버의 웹 응답을 보존하는 웹 프록시를 활용하여 로드 및 대기 시간을 효과적으로 줄인다. 클라이언트 측 웹 캐싱에는 이전에 방문한 웹 콘텐츠의 캐싱된 버전을 유지하는 브라우저 기반 캐싱이 포함될 수 있다.

![image](https://user-images.githubusercontent.com/62053017/225205581-abcfed93-5cd2-4607-b5e9-64329285eaa3.png)

## 일반 캐시

메모리에서 데이터에 액세스하면 디스크나 SSD에서 데이터에 액세스하는 것보다 훨씬 빠르기 때문에 캐시의 데이터를 활용하면 많은 이점이 있다. 트랜잭션 데이터 지원이나 디스크 기반의 내구성이 필요하지 않은 여러 사용 사례에서 인 메모리 키-값 스토어를 독립 실행형 데이터베이스로 사용하는 방법은 고성능 애플리케이션을 구축하는 데 효과적이다. 애플리케이션 속도는 물론, 경제적인 가격으로 제공되는 높은 처리량을 통해 이점을 얻을 수 있다. 제품 그룹, 범주 목록, 프로파일 정보 등의 참조 가능한 데이터는 일반 캐시의 좋은 사용 사례다.

![image](https://user-images.githubusercontent.com/62053017/225205617-e8a94091-0d7a-4741-8eca-f9a932255b3b.png)

## 통합 캐시

통합 캐시는 오리진 데이터베이스에서 자주 액세스하는 데이터를 자동으로 캐싱하는 인 메모리 계층이다. 가장 일반적으로, 기반 데이터베이스는 데이터가 캐시에 상주하는 경우 캐시를 사용하여 인바운드 데이터베이스 요청에 대한 응답을 제공한다. 이 경우 요청 지연 시간이 단축되고 데이터베이스 엔진의 CPU 및 메모리 사용률이 감소하여 데이터베이스의 성능이 크게 향상된다. 통합 캐시의 중요한 특성 중 하나는 캐싱된 데이터가 데이터베이스 엔진이 디스크에 저장한 데이터와 일치한다는 것이다.

캐싱(Caching)과 비슷한 개념으로 버퍼링(Buffering), 스풀링(Spooling) 이 있다.

# 2. 버퍼링 (Buffering)

버퍼링은 버퍼(Buffer)라고 하는 메모리 영역에 데이터를 모았다가 사용하는 동작을 의미한다. 버퍼를 이용하는 이유로는 크게 3가지를 꼽아 볼 수 있다.

1. **생산자와 소비자 사이의 속도차이에서 오는 비효율을 극복** 하기 위함이다.
 CPU를 통해 데이터를 다루다가 다시 디스크로 쓰기 연산을 하려고 할 때, 매번 디스크 드라이브로 데이터를 넘기고 디스크로 쓰여지기를 기다린다면 CPU는 메모리에 비해 속도가 매우 느린 디스크의 동작을 항상 기다리게 된다. 버퍼링을 하게 되면, 디스크로의 작업을 버퍼에 '모아서' 한번에 수행하기 때문에 보다 효율적으로 수행할 수 있다.
이런 동작은 메모리와 디스크, 네트워크와 디스크 사이에서도 찾아볼 수 있다.

2. **서로 다른 자료 전송 사이즈가 다른 상황을 극복하기 위함** 이다.
이 역시도 데이터를 모아서 서로 다른 유닛 사이즈로 다시 만드는 과정을 거치게 된다. 데이터를 모아서 디스크에 써주는 DBMS 같은 경우에는 블록 사이즈 단위로 데이터를 쓰게 된다. 이 때, 블록 사이즈가 4KB라고 하면, 수 십 바이트씩 들어오는 쓰기 요청을 모아서(버퍼링) 하나의 블록으로 쓰게 된다.

3. **데이터 입출력의 의미를 명확하게 하기 위해 사용** 된다.
예를 들어, Unix 프로세스가 Write() 시스템 콜을 사용하여 디스크 쓰기 요청을 했다고 가정해보자. write() 시스템 콜은 애플리케이션에서 데이터를 저장하고 있는 버퍼의 내용을 커널의 버퍼로 복사를 해준다. 그리고 커널의 버퍼에 복사된 데이터는 디스크로 쓰여지게 된다. 만일 커널 버퍼를 두지 않는다면? 커널은 넘겨받은 애플리케이션의 버퍼 주소를 읽어서 디스크로 쓰려고 할 것이다. 커널이 디스크로 데이터를 쓰기 전에 애플리케이션이 버퍼의 내용을 바꾼다면, 커널은 애플리케이션이 마지막에 바꾼 데이터를 디스크로 써야 할까 아니면 write() 시스템 콜이 호출된 시점의 데이터를 디스크로 내려야 할까?
커널에 버퍼를 두는 방식을 채택하면서 이런 데이터 입출력의 copy semantic을 명확히 해줄 수 있다. 커널은 커널 버퍼에 복사된 내용을 기준으로 입출력을 수행하면 된다. 애플리케이션은 자신의 버퍼를 수정하면 된다.

# 3. 스풀링 (Spooling)

스풀링은 디바이스를 독점적으로 사용해야 하는 경우에 많이 사용한다. 대표적으로 프린터를 예로 들 수 있다. 프린터는 하나의 프로세스가 프린팅을 수행하는 동안 다른 프로세스의 요청을 처리할 수 없다. 이 경우 프린팅해야 하는 작업은 대기를 해야 하는데, 이 때 추가로 프린팅해야 하는 **데이터를 디스크의 공간을 잡아서 버퍼링** 을 해놓게 된다. 이러한 방식을 스풀링(Spooling)이라고 한다.

MS Word에서 프린터를 사용해서 10페이지 가량을 인쇄하고 있는 동안 MS PowerPoint에서 프린터 사용을 요청했을 때, 프린팅 요청은 디바이스 드라이버에서 스케줄링을 기다리며, 프린팅해야 하는 데이터는 디스크에 저장하게 된다. 이 디스크 공간을 **스풀** 이라고 한다.

이전에 수행 중이던 MS Word의 프린팅 작업이 끝나면 스풀에 저장된 다음 작업인 MS PowerPoint의 프린팅 작업이 시작되며 프린팅 할 데이터를 스풀에서 읽어 수행하게 된다.
